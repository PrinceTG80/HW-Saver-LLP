{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CheckUncheck.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Name: Tushar Gavkhare\n",
        "## Colab link:\n",
        "https://colab.research.google.com/drive/1uZOAxD2osNlOkWQPnVCz4-16Nr2eZ8yX?usp=sharing\n",
        "\n",
        "The raw images are first cropped then added manually to checked or unchecked and then used Logistic Regression to create model for prediction.\n"
      ],
      "metadata": {
        "id": "IdlJxxv3DWqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        " \n",
        "#Open the zip file in read mode\n",
        "my_zipfile = zipfile.ZipFile(\"/content/Files.zip\", mode='r')\n",
        " \n",
        "print('Extracting all file...')\n",
        " \n",
        "my_zipfile.extractall()\n",
        " \n",
        "print('Extracting Done!')\n",
        "my_zipfile.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ges2oPOTzwq4",
        "outputId": "8b85d50e-bc93-4dc9-e1d3-3e82cb61e952"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting all file...\n",
            "Extracting Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "from PIL import Image\n",
        "for i in range(1,7):\n",
        "  path = '/content/Files/Raw_Dataset/img-'+str(i)+'.jpg'\n",
        "  img = PIL.Image.open(path)\n",
        "  print(img.size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jAVY0z84pmK",
        "outputId": "523886c4-0671-4902-f65f-e6aefb423d96"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(129, 121)\n",
            "(51, 48)\n",
            "(54, 45)\n",
            "(54, 45)\n",
            "(54, 45)\n",
            "(130, 120)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "path = '/content/cropped'\n",
        "shutil.rmtree(path, ignore_errors=True)"
      ],
      "metadata": {
        "id": "YoWI942D0qgY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_giheboFzV6b"
      },
      "outputs": [],
      "source": [
        "#adding scanned quetion papers in croppped_que folder\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "os.mkdir('/content/cropped')\n",
        "def box_extraction(img_for_box_extraction_path, cropped_dir_path , id):\n",
        "  img = cv2.imread(img_for_box_extraction_path, 0)  # Read the image\n",
        "  (thresh, img_bin) = cv2.threshold(img, 128, 255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)  # Thresholding the image\n",
        "  img_bin = 255-img_bin  # Invert the image\n",
        "  cv2.imwrite(\"Image_bin.jpg\",img_bin)\n",
        "   \n",
        "  # Defining a kernel length\n",
        "  kernel_length = np.array(img).shape[1]//30\n",
        "     \n",
        "  # A verticle kernel of (1 X kernel_length), which will detect all the verticle lines from the image.\n",
        "  verticle_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, kernel_length))\n",
        "  # A horizontal kernel of (kernel_length X 1), which will help to detect all the horizontal line from the image.\n",
        "  hori_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_length, 1))\n",
        "  # A kernel of (3 X 3) ones.\n",
        "  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
        "# Morphological operation to detect verticle lines from an image\n",
        "  img_temp1 = cv2.erode(img_bin, verticle_kernel, iterations=3)\n",
        "  verticle_lines_img = cv2.dilate(img_temp1, verticle_kernel, iterations=3)\n",
        "  cv2.imwrite(\"verticle_lines.jpg\",verticle_lines_img)\n",
        "# Morphological operation to detect horizontal lines from an image\n",
        "  img_temp2 = cv2.erode(img_bin, hori_kernel, iterations=3)\n",
        "  horizontal_lines_img = cv2.dilate(img_temp2, hori_kernel, iterations=3)\n",
        "  cv2.imwrite(\"horizontal_lines.jpg\",horizontal_lines_img)\n",
        "# Weighting parameters, this will decide the quantity of an image to be added to make a new image.\n",
        "  alpha = 0.5\n",
        "  beta = 1.0 - alpha\n",
        "# This function helps to add two image with specific weight parameter to get a third image as summation of two image.\n",
        "  img_final_bin = cv2.addWeighted(verticle_lines_img, alpha, horizontal_lines_img, beta, 0.0)\n",
        "  img_final_bin = cv2.erode(~img_final_bin, kernel, iterations=2)\n",
        "  (thresh, img_final_bin) = cv2.threshold(img_final_bin, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "# For Debugging\n",
        "    # Enable this line to see verticle and horizontal lines in the image which is used to find boxes\n",
        "  cv2.imwrite(\"img_final_bin.jpg\",img_final_bin)\n",
        "    # Find contours for image, which will detect all the boxes\n",
        "  contours, _ = cv2.findContours(img_final_bin, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    # Sort all the contours by top to bottom.\n",
        "  #(contours, boundingBoxes) = sort_contours(contours, method=\"top-to-bottom\")\n",
        "  idx = 0\n",
        "  Bigimg = PIL.Image.open(img_for_box_extraction_path)\n",
        "  W,H = Bigimg.size\n",
        "  for c in contours:\n",
        "    # Returns the location and width,height for every contour\n",
        "    x, y, w, h = cv2.boundingRect(c)\n",
        "    if(idx==0): \n",
        "      xprev=x\n",
        "    # print(\"rectangle \" + str(x) +\" \" + str(y) +\" \" + str(w) + \" \" + str(h) +\" \" + str(idx) + \"c\" + str(c))\n",
        "    # If the box height is greater then 20, widht is >80, then only save it as a box in \"cropped/\" folder.if (w > 65 and h > 50) and (w < 85 and h < 80): if ((w > 55 and h > 45) and (w < 85 and h < 80))\n",
        "    # if ((w > 200 and h > 250) and (w < 400 and h < 400)):\n",
        "      #print(\"rectangle \" + str(w) + \" \" + str(h) +\" \" + str(idx) + \"c\" + str(c))\n",
        "    if((w > W/2-10 and h > H/2-12) and (w < W/2 + 10 and h < H/2+10)):\n",
        "      idx += 1\n",
        "      new_img = img[y:y+h, x:x+w]\n",
        "      xprev = x\n",
        "      cv2.imwrite(cropped_dir_path + str(id) + str(idx) + '.jpg', new_img)\n",
        "      #encodeImage(cropped_dir_path + str(idx) + '.jpg',idx)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(1,10):\n",
        "  box_extraction(\"/content/Files/Raw_Dataset/img-\"+str(1)+\".jpg\", '/content/cropped/',1)\n",
        "  box_extraction(\"/content/Files/Raw_Dataset/img-\"+str(t)+\".jpg\", '/content/cropped/',t)"
      ],
      "metadata": {
        "id": "UKcQa5yB8Mzn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "path = '/content/data'\n",
        "shutil.rmtree(path, ignore_errors=True)"
      ],
      "metadata": {
        "id": "x7hc3eKLAgXx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('data')\n",
        "os.mkdir('/content/data/checked')\n",
        "os.mkdir('/content/data/unchecked')\n",
        "checked = ['/content/cropped/11.jpg','/content/cropped/21.jpg', '/content/cropped/61.jpg']\n",
        "unchecked = ['/content/cropped/31.jpg', '/content/cropped/41.jpg' , '/content/cropped/51.jpg' , '/content/cropped/71.jpg' , '/content/cropped/81.jpg' , '/content/cropped/91.jpg']\n",
        "for i in checked:\n",
        "  img = cv2.imread(i, 0)\n",
        "  cv2.imwrite('/content/data/checked/' + i[17:],img)\n",
        "for i in unchecked:\n",
        "  img = cv2.imread(i, 0)\n",
        "  cv2.imwrite('/content/data/unchecked/' + i[17:],img)"
      ],
      "metadata": {
        "id": "c9KFxmHk1qbE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imread\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "Categories=['checked','unchecked']\n",
        "flat_data_arr=[] #input array\n",
        "target_arr=[] #output array\n",
        "datadir='data' \n",
        "#path which contains all the categories of images\n",
        "for i in Categories:\n",
        "    print(f'loading... category : {i}')\n",
        "    path=os.path.join(datadir,i)\n",
        "    for img in os.listdir(path):\n",
        "        print(img)\n",
        "        img_array=imread(os.path.join(path,img))\n",
        "        img_resized=resize(img_array,(150,150,3))\n",
        "        flat_data_arr.append(img_resized.flatten())\n",
        "        target_arr.append(Categories.index(i))\n",
        "    print(f'loaded category:{i} successfully')\n",
        "flat_data=np.array(flat_data_arr)\n",
        "target=np.array(target_arr)\n",
        "df=pd.DataFrame(flat_data) #dataframe\n",
        "df['Target']=target\n",
        "x=df.iloc[:,:-1] #input data \n",
        "y=df.iloc[:,-1] #output data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD9sijzv4np9",
        "outputId": "bbbfe489-3440-4113-91f0-e90da8d5702a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading... category : checked\n",
            "61.jpg\n",
            "21.jpg\n",
            "11.jpg\n",
            "loaded category:checked successfully\n",
            "loading... category : unchecked\n",
            "71.jpg\n",
            "41.jpg\n",
            "81.jpg\n",
            "31.jpg\n",
            "91.jpg\n",
            "51.jpg\n",
            "loaded category:unchecked successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier()"
      ],
      "metadata": {
        "id": "0QvNpses40Ks"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hyp2VF947Zc",
        "outputId": "db43e38b-234a-47f5-c3bb-a89af74c22de"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred=model.predict(x)\n",
        "print(\"The predicted Data is :\")\n",
        "print(y_pred)\n",
        "print(\"The actual data is:\")\n",
        "print(np.array(y))\n",
        "print(f\"The model is {accuracy_score(y_pred,y)*100}% accurate\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsfMBgWe4--z",
        "outputId": "7fddfaf6-9ed1-499c-ecb4-21b751b6f581"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted Data is :\n",
            "[0 0 0 1 1 1 1 1 1]\n",
            "The actual data is:\n",
            "[0 0 0 1 1 1 1 1 1]\n",
            "The model is 100.0% accurate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url=input('Enter URL of Image :')\n",
        "img=imread('data/checked/' + url)\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "img_resize=resize(img,(150,150,3))\n",
        "l=[img_resize.flatten()]\n",
        "probability=model.predict_proba(l)\n",
        "for ind,val in enumerate(Categories):\n",
        "    print(f'{val} = {probability[0][ind]*100}%')\n",
        "print(\"The predicted image is : \"+Categories[model.predict(l)[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "YnJ7z_Ty5DLp",
        "outputId": "160b7a0e-a82c-45f7-ad4f-56793500a023"
      },
      "execution_count": 44,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter URL of Image :21.jpg\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checked = 100.0%\n",
            "unchecked = 0.0%\n",
            "The predicted image is : checked\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier\n",
        "from sklearn.ensemble import ExtraTreesRegressor,ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
        "from sklearn.svm import SVR,SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn.metrics import accuracy_score,mean_squared_error"
      ],
      "metadata": {
        "id": "5YxqlpZHB7_g"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers=[['Logistic Regression :',LogisticRegression()],\n",
        "       ['Decision Tree Classification :',DecisionTreeClassifier()],\n",
        "       ['Random Forest Classification :',RandomForestClassifier()],\n",
        "       ['Gradient Boosting Classification :', GradientBoostingClassifier()],\n",
        "       ['Ada Boosting Classification :',AdaBoostClassifier()],\n",
        "       ['Extra Tree Classification :', ExtraTreesClassifier()],\n",
        "       ['K-Neighbors Classification :',KNeighborsClassifier()],\n",
        "       ['Gausian Naive Bayes :',GaussianNB()]]\n",
        "cla_pred=[]\n",
        "for name,model in classifiers:\n",
        "    model=model\n",
        "    model.fit(x,y)\n",
        "    predictions = model.predict(x)\n",
        "    cla_pred.append(accuracy_score(y,predictions))\n",
        "    print(name,accuracy_score(y,predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwdC5GjgB-QS",
        "outputId": "7aada490-2993-4415-bf50-ba4c56df979f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression : 1.0\n",
            "Decision Tree Classification : 1.0\n",
            "Random Forest Classification : 1.0\n",
            "Gradient Boosting Classification : 1.0\n",
            "Ada Boosting Classification : 1.0\n",
            "Extra Tree Classification : 1.0\n",
            "K-Neighbors Classification : 0.6666666666666666\n",
            "Gausian Naive Bayes : 1.0\n"
          ]
        }
      ]
    }
  ]
}